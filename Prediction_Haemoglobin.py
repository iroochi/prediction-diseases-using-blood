# -*- coding: utf-8 -*-
"""Haemoglobin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ObH--dKaMK0HyZ1CsTvKtY_E-O3B8bsV

Artificial Neural Network

Importing the dependencies
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf

"""Loading the CSV file into data frame using pandas"""

haemo_df = pd.read_csv('/content/Haemoglobin.csv')
haemo_df

print(haemo_df.columns)

haemo_df['Sex'].replace('F', 0, inplace = True)
haemo_df['Sex'].replace('M', 1, inplace = True)
haemo_df

"""Understanding the data"""

haemo_df.shape

haemo_df.head()

haemo_df.tail()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
h1 = haemo_df['Haemoglobin(g/dl)']
h2 = haemo_df['R.B.C count(Mill/Cumm)']
legend = ['Haemoglobin', 'RBC']
plt.hist([h1,h2], color = ['red','green'])
plt.xlabel('Level')
plt.ylabel('No. of Patients')
plt.legend(legend)
plt.title('Haemoglobin vs R.B.C')
plt.show()

haemo_df.describe()

f, ax = plt.subplots(1,2,figsize = (10,5))
haemo_df['Target/Outcome'].value_counts().plot.pie(explode = [0,0.1], autopct = "%1.1f%%", ax = ax[0], shadow = True)
ax[0].set_title('Outcome')
ax[0].set_ylabel('')
sns.countplot('Target/Outcome', data = haemo_df, ax = ax[1])
ax[1].set_title('Outcome')
N, P = haemo_df['Target/Outcome'].value_counts()
print('Negative(0): ', N)
print('Positive(1): ', P)
plt.grid()
plt.show()

haemo_df.hist(bins = 10, figsize = (10,10))
plt.show()

correlation = haemo_df.corr()

correlation

sns.heatmap(correlation, annot = True)

"""Splitting the Target and other columns"""

x = haemo_df.drop(columns = 'Target/Outcome', axis = 1)
y = haemo_df['Target/Outcome']

print(x)

print(y)

"""Splitting the data into Training data and Test data"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=0)

print(x.shape, x_train.shape, x_test.shape)

print(x_train)

print(x_test)

"""Fitting and Evaluation"""

#Performing Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

print(x_test)

"""Initialization ANN(Artificial Neural Network)"""

#Initialising ANN
ann_haemo = tf.keras.models.Sequential()

#Adding First Hidden Layer
ann_haemo.add(tf.keras.layers.Dense(units=6,activation="relu"))

#Adding Second Hidden Layer
ann_haemo.add(tf.keras.layers.Dense(units=6,activation="relu"))

#Adding Output Layer
ann_haemo.add(tf.keras.layers.Dense(units=1,activation="sigmoid"))

#Compiling ANN
ann_haemo.compile(optimizer="adam",loss="binary_crossentropy",metrics=['accuracy'])

#Fitting ANN
ann_haemo.fit(x_train,y_train,batch_size=32,epochs = 100)

y_log = ann_haemo.predict(x_test)

y_pred = np.where(y_log>0.5,1,0)

from sklearn.metrics import accuracy_score
ann = accuracy_score(y_test, y_pred)
ann

ann_accuracy = accuracy_score(y_test, y_pred)*100
print("Accuracy of the Test data: " + str(round(ann_accuracy, 2))+ '%')

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
model = LogisticRegression()
# training the logistic regression model with training data
model.fit(x_train, y_train)

# accuracy score
# training data trd
x_train_pred = model.predict(x_train)
trd_accuracy = accuracy_score(x_train_pred, y_train)*100
print("Accuracy of the Training data: " + str(round(trd_accuracy, 2))+ '%')

# test data ted
x_test_pred = model.predict(x_test)
ted_accuracy = accuracy_score(x_test_pred, y_test)*100
print("Accuracy of the Test data: " + str(round(ted_accuracy, 2)) + '%')

input_data = (25, 1, 14.1, 13, 4.54, 6490) # 37th record

# input data -> numpy array
ip_data_np_arr = np.asarray(input_data)

# reshaping the array
ip_data_reshaped = ip_data_np_arr.reshape(1, -1)

pred = model.predict(ip_data_reshaped)
print(pred)

"""SVM"""

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score

sc = StandardScaler()

# training the model with training dataset
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)

print(x_train)

print(x_test)

from sklearn.svm import SVC
svc = SVC(kernel = 'linear', random_state = 0)

svc.fit(x_train, y_train)

xtrain_svm_pred = svc.predict(x_train)

xtrain_svm_pred

confusion_matrix(y_train, xtrain_svm_pred)

accuracy_score(y_train, xtrain_svm_pred)
svm_train_accuracy = accuracy_score(y_train, xtrain_svm_pred)*100
print("Accuracy of training data is: " + str(round(svm_train_accuracy, 2)) + '%')

xtest_svm_pred = svc.predict(x_test)
xtest_svm_pred

confusion_matrix(y_test, xtest_svm_pred)

sns.heatmap(confusion_matrix(y_test,xtest_svm_pred), annot = True, fmt = "d")

accuracy_score(y_test, xtest_svm_pred)
svm_test_accuracy = accuracy_score(y_test, xtest_svm_pred)*100
print("Accuracy of the model or test data is: " + str(round(svm_test_accuracy, 2)) + '%')

input_data = (12, 0, 10.1, 11, 5.17, 4760) # 18th record

# input data -> numpy array
ip_data_np_arr = np.asarray(input_data)

# reshaping the array
ip_data_reshaped = ip_data_np_arr.reshape(1, -1)

pred = svc.predict(ip_data_reshaped)
print(pred)

"""Comparison between the three models"""

model_compare = pd.DataFrame({"Artificial Neural Network":ann_accuracy,
                              "Logistic Regression":ted_accuracy,
                              "Support Vector Machine":svm_test_accuracy,}, index = ["accuracy"])
model_compare

